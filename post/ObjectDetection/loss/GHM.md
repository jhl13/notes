## Gradient Harmonized Single-stage Detector Regression
阅读笔记 by **luo13**  
2020-6-1  

这篇论文是对Focal Loss的改进，回顾Focal Loss，其主要思想是降低易分样本在总体梯度中的贡献，增加难分样本的贡献，但是忽略了一个比较关键的问题，就是难分样本其实不一定完全可分，如果过分追求正确分类难分样本，可能会导致模型的退化。  

![GHM](../../../img/GHM/gradient_norm.png)   
图一是CE训练出的模型的归一化梯度统计，可以看到，大多数样本集中在低梯度的地方，这也是focal loss解决的问题，但是可以看到，困难样本的数量也比较多。既然我们也不想让模型过分关注难分样本，那就降低样本梯度部分集中的样本权重。  

![GHM](../../../img/GHM/范数计算.png)   
范数计算方法，这里假设p都是由sigmoid函数计算得到。  

![GHM](../../../img/GHM/GD.png)   
![GHM](../../../img/GHM/划分区间.png)   
![GHM](../../../img/GHM/权值.png)   
![GHM](../../../img/GHM/GHM-C.png)   
权重计算过程中会统计每个梯度区间的样本数量，数目越多的梯度区间对应的样本权重越低，但是需要注意两点，一、上诉公式只是针对有多个区间的情况，如果全部样本都属于一个区间，其实不能分辨样本的属性，这是要权值是1，相当于不做任何修改。二、如果每个样本划分一个区间，这样计算复杂度会很大，所以作者认为地划分区间，减少计算量。  

![GHM](../../../img/GHM/EMA.png)   
在实际实验的时候还会存在一个问题，那就是，如果一个batch中都是难分样本或者易分样本，这样就不能有效分配权重，所以作者使用了滑动平均的方式，每次更新权重是使用moving值而不是当前值。

![GHM](../../../img/GHM/纠正后的梯度.png)   
纠正后的梯度在难分和易分区域都有所下降

![GHM](../../../img/GHM/GHM-G.png)   
同样的，可以将相应的方法应用到回归损失函数中，但是如果是SL1损失则存在一个问题。模长大于阈值之后，梯度就会变成1。  

![GHM](../../../img/GHM/ASL.png)   
![GHM](../../../img/GHM/ASL-2.png)  
作者对SL1进行了改进，这里没有看懂u是什么，但这样之后可以得到我们需要的梯度。  

![GHM](../../../img/GHM/GHM-G2.png)   
![GHM](../../../img/GHM/GHM-G3.png)   
作者还提到，回归损失和分类损失是不一样的，因为在一阶段算法中回归损失只在正样本中计算，而且对于回归任务来说，即使是易分样本也很重要（coco中的mAP指标从0.5-0.95）  

实验证明，GHM-G可以降低难分样本的贡献，增加易分样本的贡献，个人认为能增加易分样本的贡献是因为易分样本本身数目比较少。  
